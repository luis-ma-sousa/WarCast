{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ffa89f-2bfa-4b34-8a0c-045d17739b39",
   "metadata": {},
   "source": [
    "# Data Cleaning and database creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070dbb5-04e1-4cec-bec7-687cc8e36429",
   "metadata": {},
   "source": [
    "### Upload to database file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a371f0bd-274a-43c1-8466-5a10b4ed5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def insert_clean_csv_to_sqlite(\n",
    "    csv_path: str,\n",
    "    db_path: str,\n",
    "    table_name: str,\n",
    "    schema: str,\n",
    "    replace_table: bool = False\n",
    "):\n",
    "    # Load the cleaned CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create table if needed (you define schema manually)\n",
    "    cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} ({schema});\")\n",
    "\n",
    "    # Choose insertion mode\n",
    "    if replace_table:\n",
    "        insertion_mode = \"replace\"\n",
    "    else:\n",
    "        insertion_mode = \"append\"\n",
    "\n",
    "    # Insert the data\n",
    "    df.to_sql(table_name, conn, if_exists=insertion_mode, index=False)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"✅ Inserted {len(df)} rows into table '{table_name}' in '{db_path}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5b6a72-ba98-4c75-80c0-920bf1d1afa5",
   "metadata": {},
   "source": [
    "## Battles list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4ad4e84-8a4e-44a3-af28-991515863244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_battles_csv(\n",
    "    csv_path: str,\n",
    "    output_csv: str = \"clean_battles.csv\"\n",
    "):\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Step 1: Clean battle_name\n",
    "    def clean_battle_name(name):\n",
    "        name = str(name)\n",
    "        if \"(\" in name and \")\" in name:\n",
    "            match = re.search(r'\\(([^)]+)\\)', name)\n",
    "            if match:\n",
    "                content = match.group(1)\n",
    "                if not content.isalpha():\n",
    "                    name = re.sub(r'\\([^)]*\\)', '', name)\n",
    "        return name.strip()\n",
    "\n",
    "    df[\"battle_name\"] = df[\"battle_name\"].apply(clean_battle_name)\n",
    "\n",
    "    # Step 2: Fill year column\n",
    "    def extract_year(row):\n",
    "        if pd.notnull(row[\"year\"]):\n",
    "            return row[\"year\"]\n",
    "        desc = str(row[\"description\"])\n",
    "        parts = desc.split('-')\n",
    "\n",
    "        if len(parts) > 1 and any(x in parts[1] for x in [\"BCE\", \"BC\"]):\n",
    "            try:\n",
    "                year_candidate = int(re.findall(r'\\d+', parts[1])[0])\n",
    "                return -1 * year_candidate\n",
    "            except:\n",
    "                return None\n",
    "        elif len(parts) > 0 and any(x in parts[0] for x in [\"BCE\", \"BC\"]):\n",
    "            match = re.search(r'(\\d+)\\s*(BCE|BC)', parts[0])\n",
    "            if match:\n",
    "                return -1 * int(match.group(1))\n",
    "        return None\n",
    "\n",
    "    df[\"year\"] = df.apply(extract_year, axis=1)\n",
    "\n",
    "    # Step 3: Clean and fill conflict — only sanitize \"war\"\n",
    "    def extract_conflict(row):\n",
    "        conflict = row.get(\"conflict\", None)\n",
    "        if pd.notnull(conflict):\n",
    "            raw = str(conflict)\n",
    "        else:\n",
    "            desc_parts = str(row[\"description\"]).split('-')\n",
    "            if len(desc_parts) > 2:\n",
    "                raw = desc_parts[2]\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        # Base cleanup\n",
    "        raw = raw.strip()\n",
    "        raw = re.sub(r'^[\\d\\?\\s]*-\\s*', '', raw)\n",
    "        raw = re.sub(r'^[\\d\\s]*\\)\\-?', '', raw)\n",
    "        raw = re.sub(r'^[\\)\\-,\\s]+', '', raw)\n",
    "        raw = re.sub(r'[\\(\\)]', '', raw)\n",
    "\n",
    "        # Normalize \"war\"\n",
    "        raw = re.sub(r'\\bwars\\b', 'war', raw, flags=re.IGNORECASE)\n",
    "        raw = re.sub(r'\\bwar war\\b', 'war', raw, flags=re.IGNORECASE)\n",
    "        raw = re.sub(r'\\bwar\\b', 'war', raw, flags=re.IGNORECASE)\n",
    "        raw = re.sub(r'\\bwar[.,\\-_:;)]*\\s*$', 'war', raw, flags=re.IGNORECASE)\n",
    "\n",
    "        # Final whitespace cleanup\n",
    "        raw = re.sub(r'\\s+', ' ', raw).strip()\n",
    "\n",
    "        return raw if raw else None\n",
    "\n",
    "    df[\"conflict\"] = df.apply(extract_conflict, axis=1)\n",
    "\n",
    "    # Step 4: Format columns and handle missing values\n",
    "    df[\"battle_id\"] = pd.to_numeric(df[\"battle_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"description\"] = df[\"description\"].replace(\"None\", pd.NA).astype(\"string\")\n",
    "    df[\"conflict\"] = df[\"conflict\"].replace(\"None\", pd.NA).astype(\"string\")\n",
    "    if \"wiki_link\" not in df.columns:\n",
    "        df[\"wiki_link\"] = pd.NA\n",
    "    df[\"wiki_link\"] = df[\"wiki_link\"].replace(\"None\", pd.NA).astype(\"string\")\n",
    "\n",
    "    # Save cleaned version to CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"✅ Cleaned CSV saved as '{output_csv}' with {len(df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef44c33f-26aa-4e67-944a-b448673f452b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned CSV saved as 'clean_battles.csv' with 962 rows.\n"
     ]
    }
   ],
   "source": [
    "clean_battles_csv('/Users/louis/Desktop/Coding/Github/WarCast/Data/Pre_clean/battles_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea999e2c-4f1c-44c3-adb7-aa49496b15cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "2ad2046a-6e58-4546-a54a-444c95938dd1",
   "metadata": {},
   "source": [
    "#### Manual cleaning of very specific formatting in the 'conflict' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba8fe5c-d49b-495a-9e8f-c77453f84198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac564626-343f-405c-8875-0c8d21d23b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 962 rows into table 'battles' in '/Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db'.\n"
     ]
    }
   ],
   "source": [
    "insert_clean_csv_to_sqlite(\n",
    "    csv_path=\"/Users/louis/Desktop/Coding/Github/WarCast/Data/Clean/clean_battles.csv\",\n",
    "    db_path=\"/Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db\",\n",
    "    table_name=\"battles\",\n",
    "    schema=\"\"\"\n",
    "        battle_id INTEGER PRIMARY KEY,\n",
    "        battle_name TEXT,\n",
    "        year INTEGER,\n",
    "        description TEXT,\n",
    "        conflict TEXT,\n",
    "        wiki_link TEXT\n",
    "    \"\"\",\n",
    "    replace_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627392a3-0566-4ed8-b765-dbc512a8565b",
   "metadata": {},
   "source": [
    "## Battle info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0360bfb-1dae-4358-ade3-a0a263dcec49",
   "metadata": {},
   "source": [
    "### Primary key resetting and column formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ce5e6b-d118-4390-a8d0-2014bc44ed55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from battle_info.csv...\n",
      "Loaded 3184 rows and 8 columns\n",
      "\n",
      "Cleaning location data...\n",
      "Missing values before cleaning - Cities: 0, Countries: 383\n",
      "Missing values after cleaning - Cities: 2, Countries: 383\n",
      "\n",
      "Enforcing data types...\n",
      "\n",
      "✅ Cleaned participant data saved to 'clean_battle_info.csv' with 3184 rows.\n",
      "\n",
      "Location data statistics:\n",
      "- Rows with valid city: 3182 (99.9%)\n",
      "- Rows with valid country: 2801 (88.0%)\n",
      "- Rows with both city and country: 2799 (87.9%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_participants_csv(\n",
    "    csv_path: str = \"battle_info.csv\",\n",
    "    output_path: str = \"clean_battle_info.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Clean the battle participants CSV file, with special focus on location data.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the input CSV file\n",
    "        output_path: Path to save the cleaned CSV file\n",
    "    \"\"\"\n",
    "    # Load CSV\n",
    "    print(f\"Loading data from {csv_path}...\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    original_shape = df.shape\n",
    "    print(f\"Loaded {original_shape[0]} rows and {original_shape[1]} columns\")\n",
    "    \n",
    "    # Step 1: Reset `participant_id` column\n",
    "    if \"participant_id\" in df.columns:\n",
    "        df.drop(columns=[\"participant_id\"], inplace=True)\n",
    "    df.insert(0, \"participant_id\", df.index + 1)\n",
    "    \n",
    "    # Step 2: Clean city and country location columns\n",
    "    print(\"\\nCleaning location data...\")\n",
    "    \n",
    "    # Before counts\n",
    "    city_na_before = df['city_battle_location'].isna().sum()\n",
    "    country_na_before = df['country_battle_location'].isna().sum()\n",
    "    print(f\"Missing values before cleaning - Cities: {city_na_before}, Countries: {country_na_before}\")\n",
    "    \n",
    "    # Pre-clean coordinates from raw data\n",
    "    df['city_battle_location'] = df['city_battle_location'].apply(remove_coordinate_patterns)\n",
    "    df['country_battle_location'] = df['country_battle_location'].apply(remove_coordinate_patterns)\n",
    "    \n",
    "    # Clean locations and extract city/country when possible\n",
    "    location_pairs = []\n",
    "    for i, row in df.iterrows():\n",
    "        city, country = extract_city_country(row['city_battle_location'], row['country_battle_location'])\n",
    "        location_pairs.append((city, country))\n",
    "    \n",
    "    # Apply the extracted city and country values\n",
    "    df['city_battle_location'] = [pair[0] for pair in location_pairs]\n",
    "    df['country_battle_location'] = [pair[1] for pair in location_pairs]\n",
    "    \n",
    "    # Final sanitization of city and country names\n",
    "    df['city_battle_location'] = df['city_battle_location'].apply(sanitize_name)\n",
    "    df['country_battle_location'] = df['country_battle_location'].apply(sanitize_name)\n",
    "    \n",
    "    # Extra pass to clean up any remaining coordinate patterns\n",
    "    df['city_battle_location'] = df['city_battle_location'].apply(remove_coordinate_patterns)\n",
    "    df['country_battle_location'] = df['country_battle_location'].apply(remove_coordinate_patterns)\n",
    "    \n",
    "    # Remove unwanted geographic terms like \"Littoral\"\n",
    "    df['city_battle_location'] = df['city_battle_location'].apply(remove_geographic_terms)\n",
    "    df['country_battle_location'] = df['country_battle_location'].apply(remove_geographic_terms)\n",
    "    \n",
    "    # Final standardization of country names\n",
    "    df['country_battle_location'] = df['country_battle_location'].apply(standardize_country_name)\n",
    "    \n",
    "    # After counts\n",
    "    city_na_after = df['city_battle_location'].isna().sum()\n",
    "    country_na_after = df['country_battle_location'].isna().sum()\n",
    "    print(f\"Missing values after cleaning - Cities: {city_na_after}, Countries: {country_na_after}\")\n",
    "    \n",
    "    # Step 3: Enforce specific formats for other columns\n",
    "    print(\"\\nEnforcing data types...\")\n",
    "    df[\"participant_id\"] = pd.to_numeric(df[\"participant_id\"], downcast=\"integer\")\n",
    "    df[\"battle_id\"] = pd.to_numeric(df[\"battle_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"country\"] = df[\"country\"].astype(\"string\")\n",
    "    df[\"troops\"] = pd.to_numeric(df[\"troops\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"deaths\"] = pd.to_numeric(df[\"deaths\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"result\"] = df[\"result\"].astype(\"string\")\n",
    "    \n",
    "    # Convert location columns to string type\n",
    "    df[\"city_battle_location\"] = df[\"city_battle_location\"].astype(\"string\")\n",
    "    df[\"country_battle_location\"] = df[\"country_battle_location\"].astype(\"string\")\n",
    "    \n",
    "    # Step 4: Save cleaned file\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✅ Cleaned participant data saved to '{output_path}' with {len(df)} rows.\")\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"\\nLocation data statistics:\")\n",
    "    print(f\"- Rows with valid city: {df['city_battle_location'].notna().sum()} ({df['city_battle_location'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"- Rows with valid country: {df['country_battle_location'].notna().sum()} ({df['country_battle_location'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"- Rows with both city and country: {(df['city_battle_location'].notna() & df['country_battle_location'].notna()).sum()} ({(df['city_battle_location'].notna() & df['country_battle_location'].notna()).sum()/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Return the cleaned dataframe\n",
    "    return df\n",
    "\n",
    "def extract_city_country(city_val, country_val):\n",
    "    \"\"\"\n",
    "    Extract city and country from location values, handling complex cases.\n",
    "    \n",
    "    For example, with \"Garibpur, Dhaka, Bangladesh\", it will extract\n",
    "    \"Dhaka\" as the city (second-to-last part) and \"Bangladesh\" as the country (last part).\n",
    "    \"\"\"\n",
    "    # Common countries list for validation\n",
    "    common_countries = [\n",
    "        \"Afghanistan\", \"Albania\", \"Algeria\", \"Argentina\", \"Australia\", \"Austria\", \"Bangladesh\",\n",
    "        \"Belgium\", \"Brazil\", \"Bulgaria\", \"Canada\", \"China\", \"Croatia\", \"Czech Republic\", \n",
    "        \"Denmark\", \"Egypt\", \"Finland\", \"France\", \"Germany\", \"Greece\", \"Hungary\", \"India\", \n",
    "        \"Indonesia\", \"Iran\", \"Iraq\", \"Ireland\", \"Israel\", \"Italy\", \"Japan\", \"Korea\", \n",
    "        \"Malaysia\", \"Mexico\", \"Morocco\", \"Netherlands\", \"New Zealand\", \"Norway\", \"Pakistan\", \n",
    "        \"Philippines\", \"Poland\", \"Portugal\", \"Romania\", \"Russia\", \"Saudi Arabia\", \"Serbia\", \n",
    "        \"Singapore\", \"South Africa\", \"Spain\", \"Sweden\", \"Switzerland\", \"Syria\", \"Thailand\", \n",
    "        \"Turkey\", \"Ukraine\", \"United Arab Emirates\", \"United Kingdom\", \"Great Britain\", \"England\",\n",
    "        \"United States\", \"America\", \"Vietnam\", \"Yemen\", \"Zimbabwe\", \"Soviet Union\", \n",
    "        \"Ottoman Empire\", \"Prussia\", \"Holy Roman Empire\", \"Byzantine Empire\", \"Roman Empire\",\n",
    "        # Add countries with directional components\n",
    "        \"South Sudan\", \"East Timor\", \"North Macedonia\", \"North Korea\", \"South Korea\", \n",
    "        \"Western Sahara\", \"Eastern Republic of Uruguay\", \"United Kingdom of Great Britain and Northern Ireland\"\n",
    "    ]\n",
    "    \n",
    "    # US States list for standardization\n",
    "    us_states = [\n",
    "        \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \n",
    "        \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \n",
    "        \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \n",
    "        \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \n",
    "        \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \n",
    "        \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n",
    "        \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \n",
    "        \"Wisconsin\", \"Wyoming\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize results with the original values (possibly NaN)\n",
    "    city_result = city_val\n",
    "    country_result = country_val\n",
    "    \n",
    "    # First clean the values\n",
    "    city_cleaned = clean_location_text(city_result)\n",
    "    country_cleaned = clean_location_text(country_result)\n",
    "    \n",
    "    # If a US state is in the country column, replace with \"United States\"\n",
    "    if pd.notna(country_cleaned) and country_cleaned in us_states:\n",
    "        country_cleaned = \"United States\"\n",
    "    \n",
    "    # If one is None but the other has a comma, try to extract both from the one value\n",
    "    if pd.notna(city_cleaned) and ',' in city_cleaned and pd.isna(country_cleaned):\n",
    "        # City has multiple parts, try to extract both from it\n",
    "        parts = [p.strip() for p in city_cleaned.split(',')]\n",
    "        \n",
    "        # 1. Try to identify a country in the parts\n",
    "        country_from_parts = None\n",
    "        for part in reversed(parts):  # Check from the end\n",
    "            if part in common_countries:\n",
    "                country_from_parts = part\n",
    "                break\n",
    "        \n",
    "        if country_from_parts:\n",
    "            country_result = country_from_parts\n",
    "            # Get the index of the country\n",
    "            country_index = next((i for i, part in enumerate(parts) if part == country_from_parts), -1)\n",
    "            \n",
    "            # If it's the last part and there's at least one part before it\n",
    "            if country_index == len(parts) - 1 and country_index > 0:\n",
    "                # Use the second-to-last part as the city\n",
    "                city_result = parts[country_index - 1]\n",
    "            else:\n",
    "                # Otherwise use the first part as the city\n",
    "                city_result = parts[0]\n",
    "        else:\n",
    "            # No country found, assume last part is country and part before is city\n",
    "            if len(parts) >= 2:\n",
    "                country_result = parts[-1]\n",
    "                city_result = parts[-2]\n",
    "            else:\n",
    "                city_result = parts[0]\n",
    "    \n",
    "    elif pd.notna(country_cleaned) and ',' in country_cleaned and pd.isna(city_cleaned):\n",
    "        # Country has multiple parts, try to extract both from it\n",
    "        parts = [p.strip() for p in country_cleaned.split(',')]\n",
    "        \n",
    "        # Assume last part is country and part before is city\n",
    "        if len(parts) >= 2:\n",
    "            country_result = parts[-1]\n",
    "            city_result = parts[-2]\n",
    "        else:\n",
    "            country_result = parts[0]\n",
    "    \n",
    "    # Final cleaning of the individual values\n",
    "    city_result = clean_city_name(city_result)\n",
    "    country_result = clean_country_name(country_result)\n",
    "    \n",
    "    # If city is a country and country is None, swap them\n",
    "    if pd.notna(city_result) and pd.isna(country_result) and city_result in common_countries:\n",
    "        country_result = city_result\n",
    "        city_result = None\n",
    "    \n",
    "    # If both have values but city is a country and country isn't, swap them\n",
    "    if pd.notna(city_result) and pd.notna(country_result):\n",
    "        if city_result in common_countries and country_result not in common_countries:\n",
    "            temp = city_result\n",
    "            city_result = country_result\n",
    "            country_result = temp\n",
    "    \n",
    "    # If country is a US state, convert to United States\n",
    "    if pd.notna(country_result) and country_result in us_states:\n",
    "        country_result = \"United States\"\n",
    "    \n",
    "    # Additional cleaning to handle remaining coordinate patterns\n",
    "    city_result = remove_coordinate_patterns(city_result)\n",
    "    country_result = remove_coordinate_patterns(country_result)\n",
    "    \n",
    "    return city_result, country_result\n",
    "\n",
    "def remove_coordinate_patterns(text):\n",
    "    \"\"\"\n",
    "    Very aggressively remove any coordinate-like patterns from text.\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "        \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Check specifically for the \"N E\" pattern at the end (without periods)\n",
    "    simple_end_pattern = r'^(.*?)(?:\\s+[NESW]\\s+[NESW](?:\\s|$)).*$'\n",
    "    simple_match = re.search(simple_end_pattern, text)\n",
    "    if simple_match and simple_match.group(1).strip():\n",
    "        return simple_match.group(1).strip()\n",
    "    \n",
    "    # Check for coordinate-like patterns\n",
    "    coordinate_markers = ['.N', '.E', '.S', '.W', ' N ', ' E ', ' S ', ' W ']\n",
    "    if any(marker in text for marker in coordinate_markers) or text.rstrip().endswith((' N', ' E', ' S', ' W')):\n",
    "        # Check for repeated patterns like '.N .W' or '.N .E'\n",
    "        patterns = [\n",
    "            r'\\.[NESW]\\s+\\.[NESW]',  # Pattern like '.N .W'\n",
    "            r'\\s[NESW]\\s+\\.[NESW]',  # Pattern like 'N .W'\n",
    "            r'\\.[NESW]\\s+[NESW]\\s',  # Pattern like '.N W'\n",
    "            r'\\s[NESW]\\s+[NESW]\\s',  # Pattern like 'N W'\n",
    "            r'\\s[NESW]\\s+[NESW]$',   # Pattern like 'N E' at the end\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            if re.search(pattern, text):\n",
    "                # Find where the pattern starts\n",
    "                match = re.search(pattern, text)\n",
    "                if match:\n",
    "                    # Return only the text before the pattern\n",
    "                    return text[:match.start()].strip()\n",
    "        \n",
    "        # Look for all compass directions with periods or spaces around them\n",
    "        pattern_all_dirs = r'(?<!\\w)[\\.\\s][NESW][\\.\\s-]'\n",
    "        if re.search(pattern_all_dirs, text):\n",
    "            # Find the first occurrence\n",
    "            match = re.search(pattern_all_dirs, text)\n",
    "            if match:\n",
    "                # Return only the text before the first occurrence\n",
    "                return text[:match.start()].strip()\n",
    "                \n",
    "        # Check for single compass direction at the end\n",
    "        end_dir_pattern = r'^(.*?)\\s+[NESW]$'\n",
    "        end_match = re.search(end_dir_pattern, text)\n",
    "        if end_match:\n",
    "            return end_match.group(1).strip()\n",
    "    \n",
    "    # Look for any standalone compass directions at the end of the text\n",
    "    # This catches simpler cases like \"Thessaly .N\"\n",
    "    for dir_letter in ['N', 'E', 'S', 'W']:\n",
    "        pattern = rf'(.*?)\\s+\\.{dir_letter}.*'\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    # Remove any trailing dash which might be part of coordinate notation\n",
    "    text = re.sub(r'\\s+-$', '', text)\n",
    "    \n",
    "    # Remove single periods, which might be part of coordinate notation\n",
    "    text = re.sub(r'\\s+\\.\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_location_text(loc_val):\n",
    "    \"\"\"Initial cleaning of location text.\"\"\"\n",
    "    if pd.isna(loc_val):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string\n",
    "    loc_val = str(loc_val)\n",
    "    \n",
    "    # Remove coordinate patterns first\n",
    "    loc_val = remove_coordinate_patterns(loc_val)\n",
    "    if pd.isna(loc_val):\n",
    "        return None\n",
    "    \n",
    "    # Special case: If the text contains coordinates or special symbols,\n",
    "    # extract just the place name at the beginning\n",
    "    # We'll use a more aggressive approach to identify place names\n",
    "    if any(symbol in loc_val for symbol in ['°', '¬', '‚', 'Ä', 'Ô', 'ª', 'ø', '.N', '.E', '.S', '.W']):\n",
    "        # Find the first occurrence of degree symbol, coordinates or special characters\n",
    "        coord_indicators = ['°', '¬', '‚', 'Ä', 'Ô', 'ª', 'ø']\n",
    "        \n",
    "        # First try to find the index where coordinates start\n",
    "        coord_pos = len(loc_val)  # Default to end of string\n",
    "        for indicator in coord_indicators:\n",
    "            if indicator in loc_val:\n",
    "                pos = loc_val.find(indicator)\n",
    "                if pos < coord_pos:\n",
    "                    coord_pos = pos\n",
    "        \n",
    "        # Look for patterns like \".N\" or \".E\"\n",
    "        direction_patterns = ['.N', '.E', '.S', '.W']\n",
    "        for pattern in direction_patterns:\n",
    "            if pattern in loc_val:\n",
    "                pos = loc_val.find(pattern)\n",
    "                if pos < coord_pos:\n",
    "                    coord_pos = pos\n",
    "        \n",
    "        # If we found any coordinate indicators, extract the part before them\n",
    "        if coord_pos < len(loc_val):\n",
    "            cleaned = loc_val[:coord_pos].strip()\n",
    "            # If we have something, return it\n",
    "            if cleaned:\n",
    "                return cleaned\n",
    "    \n",
    "    # Continue with regular cleaning if no coordinates were found\n",
    "    # or if the extraction resulted in an empty string\n",
    "    \n",
    "    # Remove text in brackets, parentheses\n",
    "    cleaned = re.sub(r'\\([^)]*\\)', '', loc_val)\n",
    "    cleaned = re.sub(r'\\[[^\\]]*\\]', '', cleaned)\n",
    "    \n",
    "    # Remove footnote references\n",
    "    cleaned = re.sub(r'\\[\\d+\\]', '', cleaned)\n",
    "    \n",
    "    # Remove coordinate patterns more aggressively\n",
    "    # This will catch both the special symbols and the direction letters\n",
    "    coordinate_pattern = r'[¬°‚Ä≤‚Ä≥ÔªøNESW\\d\\.\\-\\+\\'\\\"°′″]+|(?<=\\s)[NESW](?=[\\s\\.,;])|(?<=\\s)[NESW]$'\n",
    "    cleaned = re.sub(coordinate_pattern, ' ', cleaned)\n",
    "    \n",
    "    # Remove various coordinate separators: slashes, periods, commas between numbers\n",
    "    cleaned = re.sub(r'[\\\\/]', ' ', cleaned)\n",
    "    \n",
    "    # Remove phrases like \"near\", \"present-day\", etc.\n",
    "    cleaned = re.sub(r'\\bnear\\b|\\bpresent-day\\b|\\bmodern-day\\b|\\bmodern\\b|\\bvicinity of\\b|\\boff\\b|\\barea\\b', '', cleaned, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove any text after irrelevant markers\n",
    "    irrelevant_markers = ['while more', 'identified as', 'km west', 'km east', 'km north', 'km south']\n",
    "    for marker in irrelevant_markers:\n",
    "        if marker in cleaned:\n",
    "            pos = cleaned.find(marker)\n",
    "            cleaned = cleaned[:pos].strip()\n",
    "    \n",
    "    # Remove common prefixes/suffixes\n",
    "    cleaned = re.sub(r'^the\\s+', '', cleaned, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove excess whitespace\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
    "    \n",
    "    # Replace multiple periods or dots\n",
    "    cleaned = re.sub(r'\\.{2,}', '', cleaned)\n",
    "    \n",
    "    # Final check: If the cleaned string only has punctuation or is very short, return None\n",
    "    if not re.search(r'[a-zA-Z]', cleaned) or len(cleaned) <= 1:\n",
    "        return None\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "def clean_city_name(city):\n",
    "    \"\"\"Clean city name.\"\"\"\n",
    "    if pd.isna(city):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string\n",
    "    city = str(city)\n",
    "    \n",
    "    # Remove coordinate patterns first\n",
    "    city = remove_coordinate_patterns(city)\n",
    "    if pd.isna(city):\n",
    "        return None\n",
    "    \n",
    "    # Aggressively remove coordinate and special characters\n",
    "    city = re.sub(r'[¬°‚Ä≤‚Ä≥Ôªø]', '', city)\n",
    "    \n",
    "    # Remove standalone direction indicators (N,E,S,W)\n",
    "    city = re.sub(r'\\s+[NESW]\\s+|\\s+[NESW]$|^[NESW]\\s+', ' ', city)\n",
    "    \n",
    "    # Remove digits if not all digits\n",
    "    if not city.isdigit() and any(c.isdigit() for c in city):\n",
    "        city = ''.join(c for c in city if not c.isdigit())\n",
    "        city = re.sub(r'\\s+', ' ', city).strip()\n",
    "    \n",
    "    # Set to None if it's only digits\n",
    "    if city.isdigit():\n",
    "        return None\n",
    "    \n",
    "    # Clean up directional prefixes (for cities, not countries)\n",
    "    city = re.sub(r'^(northern|southern|eastern|western|north|south|east|west|upper|lower|southeastern|southwestern|northeastern|northwestern)\\s+', '', city, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove multiple periods or dots\n",
    "    city = re.sub(r'\\.{2,}', '', city)\n",
    "    \n",
    "    # Set to None if too short after cleaning\n",
    "    if len(city) <= 1:\n",
    "        return None\n",
    "        \n",
    "    return city\n",
    "\n",
    "def clean_country_name(country):\n",
    "    \"\"\"Clean country name.\"\"\"\n",
    "    if pd.isna(country):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string\n",
    "    country = str(country)\n",
    "    \n",
    "    # Remove coordinate patterns first\n",
    "    country = remove_coordinate_patterns(country)\n",
    "    if pd.isna(country):\n",
    "        return None\n",
    "    \n",
    "    # Aggressively remove coordinate and special characters\n",
    "    country = re.sub(r'[¬°‚Ä≤‚Ä≥Ôªø]', '', country)\n",
    "    \n",
    "    # Remove standalone direction indicators (N,E,S,W)\n",
    "    country = re.sub(r'\\s+[NESW]\\s+|\\s+[NESW]$|^[NESW]\\s+', ' ', country)\n",
    "    \n",
    "    # List of valid country names with directional components\n",
    "    directional_countries = [\n",
    "        \"South Africa\", \"South Sudan\", \"East Timor\", \"North Macedonia\", \n",
    "        \"North Korea\", \"South Korea\", \"Western Sahara\",\n",
    "        \"Eastern Republic of Uruguay\", \"United Kingdom of Great Britain and Northern Ireland\"\n",
    "    ]\n",
    "    \n",
    "    # Check if the country is already a recognized country with direction\n",
    "    for valid_country in directional_countries:\n",
    "        if valid_country.lower() in country.lower():\n",
    "            # Return the standardized form\n",
    "            return valid_country\n",
    "    \n",
    "    # US States list for standardization\n",
    "    us_states = [\n",
    "        \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \n",
    "        \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \n",
    "        \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \n",
    "        \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \n",
    "        \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \n",
    "        \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n",
    "        \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \n",
    "        \"Wisconsin\", \"Wyoming\"\n",
    "    ]\n",
    "    \n",
    "    # Replace US States with \"United States\"\n",
    "    if country in us_states:\n",
    "        return \"United States\"\n",
    "    \n",
    "    # Remove digits if not all digits\n",
    "    if not country.isdigit() and any(c.isdigit() for c in country):\n",
    "        country = ''.join(c for c in country if not c.isdigit())\n",
    "        country = re.sub(r'\\s+', ' ', country).strip()\n",
    "    \n",
    "    # Set to None if it's only digits\n",
    "    if country.isdigit():\n",
    "        return None\n",
    "    \n",
    "    # Clean up directional prefixes ONLY for non-recognized countries\n",
    "    if not any(valid_country.lower() in country.lower() for valid_country in directional_countries):\n",
    "        country = re.sub(r'^(northern|southern|eastern|western|north|south|east|west|upper|lower|southeastern|southwestern|northeastern|northwestern)\\s+', '', country, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Remove multiple periods or dots\n",
    "    country = re.sub(r'\\.{2,}', '', country)\n",
    "    \n",
    "    # Set to None if too short after cleaning\n",
    "    if len(country) <= 1:\n",
    "        return None\n",
    "    \n",
    "    # Standardize country names\n",
    "    country = standardize_country_name(country)\n",
    "        \n",
    "    return country\n",
    "\n",
    "def standardize_country_name(country):\n",
    "    \"\"\"Standardize country names to ensure consistency.\"\"\"\n",
    "    if pd.isna(country):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string\n",
    "    country = str(country)\n",
    "    \n",
    "    # Replace common variations/abbreviations\n",
    "    country_replacements = {\n",
    "        \"US\": \"United States\",\n",
    "        \"U.S.\": \"United States\",\n",
    "        \"U.S.A.\": \"United States\",\n",
    "        \"USA\": \"United States\",\n",
    "        \"America\": \"United States\",\n",
    "        \"UK\": \"United Kingdom\",\n",
    "        \"U.K.\": \"United Kingdom\",\n",
    "        \"Britain\": \"United Kingdom\",\n",
    "        \"Great Britain\": \"United Kingdom\",\n",
    "        \"England\": \"United Kingdom\", \n",
    "        \"Soviet Union\": \"Russia\",\n",
    "        \"USSR\": \"Russia\",\n",
    "        \"U.S.S.R.\": \"Russia\"\n",
    "    }\n",
    "    \n",
    "    # Check for exact matches\n",
    "    if country in country_replacements:\n",
    "        return country_replacements[country]\n",
    "    \n",
    "    # Check for case-insensitive matches\n",
    "    for key, value in country_replacements.items():\n",
    "        if country.lower() == key.lower():\n",
    "            return value\n",
    "    \n",
    "    return country\n",
    "\n",
    "def remove_geographic_terms(name):\n",
    "    \"\"\"Remove unwanted geographic terms from location names.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string\n",
    "    name = str(name)\n",
    "    \n",
    "    # List of geographic terms to remove\n",
    "    geographic_terms = [\n",
    "        r'\\blittoral\\b',\n",
    "        r'\\bplain\\b',\n",
    "        r'\\bplateau\\b',\n",
    "        r'\\bbasin\\b',\n",
    "        r'\\bdelta\\b',\n",
    "        r'\\bestuary\\b',\n",
    "        r'\\bpeninsula\\b',\n",
    "        r'\\bisthmus\\b',\n",
    "        r'\\bstrait\\b'\n",
    "    ]\n",
    "    \n",
    "    # Remove each term (case insensitive)\n",
    "    for term in geographic_terms:\n",
    "        name = re.sub(term, '', name, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Clean up any resulting extra spaces\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    name = re.sub(r',\\s*$', '', name)  # Remove trailing comma\n",
    "    name = re.sub(r'^\\s*,', '', name)  # Remove leading comma\n",
    "    \n",
    "    # Convert to None if empty\n",
    "    if not name:\n",
    "        return None\n",
    "    \n",
    "    return name\n",
    "\n",
    "def sanitize_name(name):\n",
    "    \"\"\"Final sanitization of names to ensure they are clean and standardized.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    \n",
    "    # Convert to string\n",
    "    name = str(name)\n",
    "    \n",
    "    # Look for and remove patterns of standalone compass directions\n",
    "    name = remove_coordinate_patterns(name)\n",
    "    if pd.isna(name):\n",
    "        return None\n",
    "    \n",
    "    # Trim whitespace\n",
    "    name = name.strip()\n",
    "    \n",
    "    # Remove any standalone compass directions (N, E, S, W)\n",
    "    name = re.sub(r'\\s+[NESW](?=\\s|$)', '', name)\n",
    "    name = re.sub(r'\\s+[NESW]\\s+', ' ', name)\n",
    "    name = re.sub(r'\\s+[NESW]\\.$', '', name)\n",
    "    name = re.sub(r'\\.+[NESW]', '', name)\n",
    "    \n",
    "    # Remove any remaining special characters\n",
    "    name = re.sub(r'[^\\w\\s,.-]', '', name)\n",
    "    \n",
    "    # Remove duplicate spaces\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    \n",
    "    # Remove extra periods and trailing punctuation\n",
    "    name = re.sub(r'\\.{2,}', '.', name)\n",
    "    name = re.sub(r'[.,;:]+$', '', name).strip()\n",
    "    \n",
    "    # Convert to None if empty\n",
    "    if not name:\n",
    "        return None\n",
    "    \n",
    "    return name\n",
    "\n",
    "# Run the cleaning script if executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    clean_participants_csv(\"battle_info.csv\", \"clean_battle_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f29c3d-47de-4afb-8b64-59062dc758c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6294a864-78ff-4a6a-9bb6-4e9722c4014b",
   "metadata": {},
   "source": [
    "Manual cleaning of very specific formatting in the 'country' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c7999-bd3f-4603-8854-d3bdaec7d362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5406d54e-2f3e-4a34-ba8a-179b2db27b81",
   "metadata": {},
   "source": [
    "### Upload to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7c6bbbf-e0aa-4f0c-9ed6-cb8a4ff33f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 3184 rows into table 'battle_info' in '/Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db'.\n"
     ]
    }
   ],
   "source": [
    "insert_clean_csv_to_sqlite(\n",
    "    csv_path='/Users/louis/Desktop/Coding/Github/WarCast/Data/Clean/clean_battle_info.csv',\n",
    "    db_path='/Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db',\n",
    "    table_name=\"battle_info\",\n",
    "    schema=\"\"\"\n",
    "        participant_id INTEGER PRIMARY KEY,\n",
    "        battle_id INTEGER,\n",
    "        country TEXT,\n",
    "        troops INTEGER,\n",
    "        deaths INTEGER,\n",
    "        result TEXT,\n",
    "        city_battle_location TEXT,\n",
    "        country_battle_location TEXT\n",
    "    \"\"\",\n",
    "    replace_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0524e7-c9f2-4967-85c7-2181ed6633b7",
   "metadata": {},
   "source": [
    "## GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90296fb1-f7e3-481a-a0dc-59e859a80e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def process_gdp_excel_to_sqlite(\n",
    "    excel_path: str,\n",
    "    db_path: str = '/Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db',\n",
    "    table_name: str = \"gdp\"\n",
    "):\n",
    "    # Step 1: Load Excel\n",
    "    df = pd.read_excel(excel_path)\n",
    "    \n",
    "    # Step 2: Melt into long format\n",
    "    df_melted = df.melt(\n",
    "        id_vars=[\"Country Name\"],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"gdp\"\n",
    "    )\n",
    "    \n",
    "    # Step 3: Clean\n",
    "    df_melted.rename(columns={\"Country Name\": \"country\"}, inplace=True)\n",
    "    df_melted = df_melted.dropna(subset=[\"gdp\"])\n",
    "    df_melted[\"year\"] = pd.to_numeric(df_melted[\"year\"], errors=\"coerce\").dropna().astype(int)\n",
    "    df_melted[\"gdp\"] = pd.to_numeric(df_melted[\"gdp\"], errors=\"coerce\").round(2)\n",
    "\n",
    "    # Step 4: Add surrogate key (optional if you're going to use AUTOINCREMENT in SQL)\n",
    "    df_melted = df_melted.sort_values(by=[\"country\", \"year\"]).reset_index(drop=True)\n",
    "    df_melted.insert(0, \"id\", df_melted.index + 1)\n",
    "\n",
    "    # Step 5: Save cleaned CSV (optional but useful for inspection)\n",
    "    df_melted.to_csv(\"clean_gdp.csv\", index=False)\n",
    "\n",
    "    # Step 6: Create or insert into SQLite\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create table (safe to run repeatedly due to IF NOT EXISTS)\n",
    "    cursor.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        country TEXT,\n",
    "        year INTEGER,\n",
    "        gdp REAL,\n",
    "        UNIQUE (country, year)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert into database\n",
    "    df_melted.to_sql(table_name, conn, if_exists=\"append\", index=False)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    print(f\"✅ Processed and saved GDP data to table '{table_name}' in {db_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5776fd0-190c-4a29-9262-0ef8e76f8931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed and saved GDP data to table 'gdp' in /Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db\n"
     ]
    }
   ],
   "source": [
    "process_gdp_excel_to_sqlite('/Users/louis/Desktop/Coding/Github/WarCast/Data/Pre_clean/GDP per country_1988-2022.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6952779-4780-4ada-b54e-fce521ed2fed",
   "metadata": {},
   "source": [
    "## Political regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4101db1-2c94-4a17-8b47-9a86854340d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Mapping from code to regime label\n",
    "REGIME_MAP = {\n",
    "    0: \"closed autocracy\",\n",
    "    1: \"electoral autocracy\",\n",
    "    2: \"electoral democracy\",\n",
    "    3: \"liberal democracy\"\n",
    "}\n",
    "\n",
    "def process_political_regime_to_sqlite(\n",
    "    csv_path: str = '',\n",
    "    db_path: str = \"/Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db\",\n",
    "    table_name: str = \"political_regime\"\n",
    "):\n",
    "    # Step 1: Load CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Step 2: Rename relevant columns\n",
    "    df = df.rename(columns={\n",
    "        \"Entity\": \"country\",\n",
    "        \"Year\": \"year\",\n",
    "        df.columns[-1]: \"regime_code\"  # Last column is assumed to be the regime score\n",
    "    })\n",
    "\n",
    "    # Step 3: Map regime code to readable labels\n",
    "    df[\"regime_type\"] = df[\"regime_code\"].map(REGIME_MAP)\n",
    "\n",
    "    # Step 4: Sort and add surrogate ID\n",
    "    df = df.sort_values(by=[\"country\", \"year\"]).reset_index(drop=True)\n",
    "    df.insert(0, \"id\", df.index + 1)\n",
    "\n",
    "    # Step 5: Save full cleaned CSV including 'Code'\n",
    "    df.to_csv(\"clean_political_regime.csv\", index=False)\n",
    "\n",
    "    # Step 6: Connect to SQLite and create table\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        country TEXT,\n",
    "        year INTEGER,\n",
    "        regime_code INTEGER,\n",
    "        regime_type TEXT,\n",
    "        UNIQUE (country, year)\n",
    "    );\n",
    "    \"\"\")\n",
    "\n",
    "    # Step 7: Insert only relevant columns into SQL\n",
    "    df_sql = df[[\"id\", \"country\", \"year\", \"regime_code\", \"regime_type\"]]\n",
    "    try:\n",
    "        df_sql.to_sql(table_name, conn, if_exists=\"append\", index=False)\n",
    "        print(f\"✅ Inserted data into table '{table_name}' in {db_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to insert data: {e}\")\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0abb6158-c257-4d89-b55f-dbb37e550bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted data into table 'political_regime' in /Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db\n"
     ]
    }
   ],
   "source": [
    "process_political_regime_to_sqlite('/Users/louis/Desktop/Coding/Github/WarCast/Data/Raw/political-regime/political-regime.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b09d7-68cd-4748-970f-751bfdb00097",
   "metadata": {},
   "source": [
    "## Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a29900e7-da6c-4e53-bf98-a3eae7c22837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_population_csv(\n",
    "    csv_path: str = \"\",\n",
    "    output_path: str = \"clean_population.csv\"\n",
    "):\n",
    "    # Load the raw file\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Step 1: Rename columns\n",
    "    df.rename(columns={\n",
    "        \"Entity\": \"Country\",\n",
    "        \"Population (historical)\": \"Population\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Step 2: Enforce data types\n",
    "    df[\"Country\"] = df[\"Country\"].astype(\"string\")\n",
    "    df[\"Code\"] = df[\"Code\"].astype(\"string\")\n",
    "    df[\"Year\"] = pd.to_numeric(df[\"Year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"Population\"] = pd.to_numeric(df[\"Population\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Step 3: Save cleaned file\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Cleaned population data saved to '{output_path}' with {len(df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3447037-7da5-49d1-b95c-4876dfa4988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned population data saved to 'clean_population.csv' with 59177 rows.\n"
     ]
    }
   ],
   "source": [
    "clean_population_csv('/Users/louis/Desktop/Coding/Github/WarCast/Raw Data/population/population.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccba942-a31a-4584-bcc1-2fa6e7849147",
   "metadata": {},
   "source": [
    "### Upload to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea939f2c-2a1d-496d-8352-8e9346b4f3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 59177 rows into table 'population' in '/Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db'.\n"
     ]
    }
   ],
   "source": [
    "insert_clean_csv_to_sqlite(\n",
    "    csv_path='/Users/louis/Desktop/Coding/Github/WarCast/Data/Clean/clean_population.csv',\n",
    "    db_path=\"/Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db\",\n",
    "    table_name=\"population\",\n",
    "    schema=\"\"\"\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        Country TEXT,\n",
    "        Code TEXT,\n",
    "        Year INTEGER,\n",
    "        Population INTEGER\n",
    "    \"\"\",\n",
    "    replace_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff2e7fb-d56c-4d41-989e-e11cdf669810",
   "metadata": {},
   "source": [
    "## Corruption Perception Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18dba2ef-bf0a-42b1-87ee-8a008287cdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_cpi_scores(\n",
    "    excel_path: str,\n",
    "    output_path: str = \"clean_cpi.csv\"\n",
    "):\n",
    "    # Load Excel\n",
    "    df = pd.read_csv(excel_path,encoding='latin1')\n",
    "\n",
    "    # Step 1: Keep only country info and CPI score columns\n",
    "    cpi_cols = [col for col in df.columns if \"CPI score\" in col]\n",
    "    base_cols = [\"Country\", \"ISO3\"]\n",
    "    df = df[base_cols + cpi_cols]\n",
    "\n",
    "    # Step 2: Melt to long format\n",
    "    df_long = df.melt(\n",
    "        id_vars=[\"Country\", \"ISO3\"],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"cpi_score\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Clean year column from 'CPI score YYYY' → YYYY\n",
    "    df_long[\"year\"] = df_long[\"year\"].str.extract(r'(\\d{4})').astype(\"Int64\")\n",
    "\n",
    "    # Step 4: Format columns\n",
    "    df_long = df_long.rename(columns={\"Country\": \"country\", \"ISO3\": \"code\"})\n",
    "    df_long[\"country\"] = df_long[\"country\"].astype(\"string\")\n",
    "    df_long[\"code\"] = df_long[\"code\"].astype(\"string\")\n",
    "    df_long[\"cpi_score\"] = pd.to_numeric(df_long[\"cpi_score\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Step 5: Add auto ID\n",
    "    df_long = df_long.sort_values(by=[\"country\", \"year\"]).reset_index(drop=True)\n",
    "    df_long.insert(0, \"id\", df_long.index + 1)\n",
    "\n",
    "    # Step 6: Save cleaned file\n",
    "    df_long.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Cleaned CPI data saved to '{output_path}' with {len(df_long)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f19ae915-6717-4685-bd06-d3a8f95b2409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned CPI data saved to 'clean_cpi.csv' with 1260 rows.\n"
     ]
    }
   ],
   "source": [
    "clean_cpi_scores('/Users/louis/Desktop/Coding/Github/WarCast/Pre-clean data/CPI2020_GlobalTablesTS_210125.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985d891-8817-4760-86a7-db370a268932",
   "metadata": {},
   "source": [
    "### Upload to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba2545bc-153e-4495-be84-a752c6869a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 1260 rows into table 'cpi' in '/Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db'.\n"
     ]
    }
   ],
   "source": [
    "insert_clean_csv_to_sqlite(\n",
    "    csv_path='/Users/louis/Desktop/Coding/Github/WarCast/Data/Clean/clean_cpi.csv',\n",
    "    db_path=\"/Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db\",\n",
    "    table_name=\"cpi\",\n",
    "    schema=\"\"\"\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        country TEXT,\n",
    "        code TEXT,\n",
    "        year INTEGER,\n",
    "        cpi_score INTEGER\n",
    "    \"\"\",\n",
    "    replace_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3414ac63-2aab-4ca1-9a14-eeff522fac3e",
   "metadata": {},
   "source": [
    "## Military investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e2b4ff0-20f1-47fa-b07d-6f02fea93c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_military_percent_gdp(\n",
    "    csv_path: str = \"Military percent GDP.csv\",\n",
    "    output_path: str = \"clean_military_gdp.csv\"\n",
    "):\n",
    "    # Load raw CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Step 1: Drop unused columns\n",
    "    df = df.drop(columns=[\"Indicator Name\", \"Indicator Code\"], errors=\"ignore\")\n",
    "\n",
    "    # Step 2: Melt into tall format\n",
    "    df_long = df.melt(\n",
    "        id_vars=[\"Country Name\", \"Country Code\"],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"military_percent_gdp\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Rename columns and enforce types\n",
    "    df_long = df_long.rename(columns={\n",
    "        \"Country Name\": \"country\",\n",
    "        \"Country Code\": \"code\"\n",
    "    })\n",
    "\n",
    "    df_long[\"country\"] = df_long[\"country\"].astype(\"string\")\n",
    "    df_long[\"code\"] = df_long[\"code\"].astype(\"string\")\n",
    "    df_long[\"year\"] = pd.to_numeric(df_long[\"year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df_long[\"military_percent_gdp\"] = pd.to_numeric(df_long[\"military_percent_gdp\"], errors=\"coerce\").astype(\"float\")\n",
    "\n",
    "    # Step 4: Add auto ID\n",
    "    df_long = df_long.sort_values(by=[\"country\", \"year\"]).reset_index(drop=True)\n",
    "    df_long.insert(0, \"id\", df_long.index + 1)\n",
    "\n",
    "    # Step 5: Save cleaned version\n",
    "    df_long.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Cleaned military GDP data saved to '{output_path}' with {len(df_long)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08102bc7-6779-40a9-9ed5-61856b616ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned military GDP data saved to 'clean_military_gdp.csv' with 17024 rows.\n"
     ]
    }
   ],
   "source": [
    "clean_military_percent_gdp('/Users/louis/Desktop/Coding/Github/WarCast/Data/Pre_clean/Military percent GDP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96041774-3ec7-4935-9f0b-5db7b7d8dd3c",
   "metadata": {},
   "source": [
    "### Upload to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "020b0a2c-b53c-4fb8-ae7f-483a49eafcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted 17024 rows into table 'gdp_military' in '/Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db'.\n"
     ]
    }
   ],
   "source": [
    "insert_clean_csv_to_sqlite(\n",
    "    csv_path='/Users/louis/Desktop/Coding/Github/WarCast/Data/Clean/clean_military_gdp.csv',\n",
    "    db_path=\"/Users/louis/Desktop/Coding/Github/WarCast/Data/Database/warcast.db\",\n",
    "    table_name=\"gdp_military\",\n",
    "    schema=\"\"\"\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        country TEXT,\n",
    "        code TEXT,\n",
    "        year INTEGER,\n",
    "        military_percent_gdp REAL\n",
    "    \"\"\",\n",
    "    replace_table=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9e39a-10fb-496e-8e1d-6c3fc4f10012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
